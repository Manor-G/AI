# -*- coding: utf-8 -*-
"""20226314 Zhang_Gary Ex 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11eDUeWJ9mTP41RWm44Xo0xAePOoigh1E

# MMAI 894 - Exercise 2
## Convolutional artificial neural network : Image classification
The goal of this excercise is to show you how to create your first neural network using the tensorflow/keras library. We will be using the MNIST dataset.

Submission instructions:
- You cannot edit this notebook directly. Save a copy to your drive, and make sure to identify yourself in the title using name and student number
- Do not insert new cells before the final one (titled "Further exploration") 
- Select File -> Download as .py (important! not as ipynb)
- Rename the file: `studentID_lastname_firstname_ex2.py`
- Notebook must be able to _restart and run all_
- The mark will be assessed on the implementation of the functions with #TODO
- **Do not change anything outside the functions**  unless in the further exploration section
- The mark is primarily based on correctness. However, as you are encouraged to explore the architecture, meeting high performance is required.

- Do not use any additional libraries than the ones listed below (you may import additional modules from those libraries if needed)
"""

# Import modules
# Add modules as needed
from sklearn.datasets import fetch_openml

# For windows laptops add following 2 lines:
# import matplotlib
# matplotlib.use('agg')

# import matplotlib.pyplot as plt

import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Input
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.models import Model
from keras.callbacks import ReduceLROnPlateau

"""### Data preparation

#### Import data
"""

def load_data():
    # Import MNIST dataset from openml
    dataset = fetch_openml('mnist_784', version=1, data_home=None)

    # Data preparation
    raw_X = dataset['data']
    raw_Y = dataset['target']
    return raw_X, raw_Y

raw_X, raw_Y = load_data()

"""## Consider the following
- Same as excercise 1
- what shape should x be for a convolutional network?
"""

def clean_data(raw_X, raw_Y):
    # TODO: clean and QA raw_X and raw_Y
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    print("Original X shape: ", raw_X.shape)
    print("Original Y shape: ", raw_Y.shape)
    cleaned_X = (raw_X / 255.0).reshape(-1, 28, 28, 1)
    num_classes = len(set(raw_Y))
    cleaned_Y = keras.utils.to_categorical(raw_Y, num_classes = num_classes)
    print("New X shape: ", cleaned_X.shape)
    print("New Y shape: ", cleaned_Y.shape)

    
    return cleaned_X, cleaned_Y

cleaned_X, cleaned_Y = clean_data(raw_X, raw_Y)

"""#### Data split

- Split your data into a train set (50%), validation set (20%) and a test set (30%). You can use scikit-learn's train_test_split function.
"""

def split_data(cleaned_X, cleaned_Y):
    # TODO: split the data
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    from sklearn.model_selection import train_test_split
    X_train, X_test, Y_train, Y_test = train_test_split(cleaned_X, cleaned_Y, train_size=0.5, random_state=42)
    X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.6, random_state=42)
    print("train_set: ", X_train.shape)
    print("val_set: ", X_val.shape)
    print("test_set: ", X_test.shape)


    
    return X_val, X_test, X_train, Y_val, Y_test, Y_train

X_val, X_test, X_train, Y_val, Y_test, Y_train = split_data(cleaned_X, cleaned_Y)

"""### Model

#### Neural network structure

This time, the exact model architecture is left to you to explore.  
Keep the number of parameters below 2,000,000
"""

def build_model():
    # TODO: build the model, 
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    i = Input(shape=X_train[0].shape)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)
    x = BatchNormalization()(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.5)(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.5)(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Flatten()(x)

    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(10, activation='softmax')(x)

    model = Model(i, x)
    
    
    return model

def compile_model(model):
    # TODO: compile the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])



    return model


def train_model(model, X_train, Y_train, X_val, Y_val):
    # TODO: train the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)
    
    history = model.fit(X_train, Y_train, batch_size =128, 
                         validation_data=(X_val, Y_val), epochs=35, 
                         callbacks=[learning_rate_reduction])
    return model, history


def eval_model(model, X_test, Y_test):
    # TODO: evaluate the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    test_loss, test_accuracy = model.evaluate(X_test, Y_test)
    from sklearn.metrics import confusion_matrix
    print("\nTest loss:{:10.3f}".format(test_loss))
    print("\nTest accuracy:{:10.3f}".format(test_accuracy))
    y_pred =model.predict(X_test).argmax(axis=1)
    cm = confusion_matrix(Y_test.argmax(axis=1), y_pred)
    print("\nConfusion Matrix\n")
    print(cm)


    return test_loss, test_accuracy

## You may use this space (and add additional cells for exploration)

model = build_model()
model = compile_model(model)
model, history = train_model(model, X_train, Y_train, X_val, Y_val)
test_loss, test_accuracy = eval_model(model, X_test, Y_test)

# TODO: try to make the format looks better